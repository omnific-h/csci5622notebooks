{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 20: K-Means and Gaussian Mixture Models\n",
    "***\n",
    "\n",
    "<img src=\"figs/gmmbanner.png\",width=1100,height=50>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reminder**:  Go to the botttom of the notebook and shift-enter the [helper functions](#helpers)\n",
    "***\n",
    "\n",
    "### Problem 1: Simple GMM and EM Example \n",
    "***\n",
    "> This problem was adopted from Chapter 6 of the book *Machine Learning* by Tom. M. Mitchell\n",
    "\n",
    "Consider the simple case where you have data drawn from a mixture of two 1D gaussians with fixed and identical variance.  Furthermore, we'll assume that there is no preference given to either of the distributions in the mixture.  The following plot shows the sampled points superimposed with the pdfs of the true distributions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu  = [-1, 1]; sig = [.5, .5]\n",
    "X = prob1Data(50, mu, sig)\n",
    "prob1Progress(X, mu, sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the data.  It's pretty clear that you can make strong inferences about which Gaussian some of the points came from.  But the ones in the middle are a little tricker (especially because the Gaussians have identical variance).  The question is, given the data and the identical variance assumption, can we use an unsupervised learning algorithm to estimate the mean?  \n",
    "\n",
    "It turns out that the EM algorithm can do just this.  Recall that EM is an iterative method that proceeds in two states \n",
    "\n",
    "$~~\\bullet~~$ (E-Step): For each $i$ and $k$, estimate the probability that $x_i$ came from the Gaussian with $\\mu_k$.  Since the two Gaussians are equally likely, we have:  \n",
    "\n",
    "$$\n",
    "r_{ik} = p(\\mu=\\mu_k ~|~ x_i) = \n",
    "\\frac{\\exp\\{-(x_i -\\mu_k)^2/2\\sigma^2 \\}}{\\textstyle\\sum_{j=1}^2\\exp\\{-(x_i -\\mu_j)^2/2\\sigma^2 \\}}\n",
    "$$\n",
    "\n",
    "$~~\\bullet~~$ (M-Step): Estimate each $\\mu_k$ using the weighted average of the training examples\n",
    "\n",
    "$$\n",
    "\\mu_k \\leftarrow \\frac{\\textstyle\\sum_{i=1}^m r_{ik}x_i}{\\textstyle\\sum_{i=1}^m r_{ik}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: The following code skeleton will compute a single iteration of the EM algorithm given a current guess for the means, stored in $\\hat{\\mu}$ to distinguish them from the $\\mu$ used to simulate the data.  Your job is to complete the code for the E-Step and M-Step of the algorithm. \n",
    "\n",
    "\n",
    "**Hint**: Make sure you check the Helper Functions at the bottom.  You'll find some helpful things already implemented. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def EM(X, muh, sig=[0.5, 0.5]):\n",
    "    \n",
    "    # E-Step to estimate r_ik's \n",
    "    R = np.zeros((X.shape[0],len(muh)))\n",
    "    for kk, (m, s) in enumerate(zip(muh, sig)):\n",
    "        R[:,kk] = # TODO \n",
    "    R = # TODO \n",
    "    \n",
    "    # M-Step to compute MLE of means \n",
    "    for kk in range(len(muh)):\n",
    "        muh[kk] = # TODO \n",
    "        \n",
    "    return muh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll define an initial guess for the $\\hat{\\mu}$'s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "muh = [-2.0, -1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've implemented the EM algorithm correctly, repeated shift-entering of the following cell should update the $\\hat{\\mu}$'s and show you your progress. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "muh = EM(X, muh, sig)\n",
    "prob1Progress(X, mu, sig, muh=muh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: About how many iterations does it take for EM to converge? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Can you find an initial guess that will cause EM to get stuck in a local minimum? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: What would you need to do to modify the code if you assume the variances are not fixed and identical? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: What would you need to do to modify the code if you assume that the gaussians are selected with different probabilities? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Problem 2: Relationship between K-Means and GMMs with EM \n",
    "***\n",
    "\n",
    "I claimed in the videos that K-Means and GMM/EM are equivalent in the case that you assume that the variances of the multivariate Gaussians are identical and assumed known and that the individual Gaussians are chosen uniformly.  In this exercise we'll prove that this is true.  \n",
    "\n",
    "**Q**: What is the objective function you're trying to minimize in K-Means? \n",
    "\n",
    "**Q**: Given a specification of cluster membership for each ${\\bf x}_i$, how do we update the means of the clusters?\n",
    "\n",
    "**Q** Under the assumptions that $\\Sigma_k = \\Sigma = \\textrm{diag}(\\sigma^2, \\ldots, \\sigma^2)$ and that we've assigned examples $\\{{\\bf x}_i\\}_{i=1}^m$ to specific clusters, what is the log-likelihood of the parameters $\\mu_k$ and $\\Sigma_k$? **Hint**: It's easier if you start with the likelihood over examples in a particular cluster.  In other words, what is $\\ell(\\mu_k, \\Sigma_k ~|~ z = k)$? \n",
    "\n",
    "**Q**: What is the update rule for the means in the EM algorithm? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<br><br><br>\n",
    "<br><br><br>\n",
    "<br><br><br>\n",
    "\n",
    "### Problem 1: Simple GMM and EM Example \n",
    "***\n",
    "> This problem was adopted from Chapter 6 of the book *Machine Learning* by Tom. M. Mitchell\n",
    "\n",
    "Consider the simple case where you have data drawn from a mixture of two 1D gaussians with fixed and identical variance.  Furthermore, we'll assume that there is no preference given to either of the distributions in the mixture.  The following plot shows the sampled points superimposed with the pdfs of the true distributions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu  = [-1, 1]; sig = [.5, .5]\n",
    "X = prob1Data(50, mu, sig)\n",
    "prob1Progress(X, mu, sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the data.  It's pretty clear that you can make strong inferences about which Gaussian some of the points came from.  But the ones in the middle are a little tricker (especially because the Gaussians have identical variance).  The question is, given the data and the identical variance assumption, can we use an unsupervised learning algorithm to estimate the mean?  \n",
    "\n",
    "It turns out that the EM algorithm can do just this.  Recall that EM is an iterative method that proceeds in two states \n",
    "\n",
    "$~~\\bullet~~$ (E-Step): For each $i$ and $k$, estimate the probability that $x_i$ came from the Gaussian with $\\mu_k$.  Since the two Gaussians are equally likely, we have:  \n",
    "\n",
    "$$\n",
    "r_{ik} = p(\\mu=\\mu_k ~|~ x_i) = \n",
    "\\frac{\\exp\\{-(x_i -\\mu_k)^2/2\\sigma^2 \\}}{\\textstyle\\sum_{j=1}^2\\exp\\{-(x_i -\\mu_j)^2/2\\sigma^2 \\}}\n",
    "$$\n",
    "\n",
    "$~~\\bullet~~$ (M-Step): Estimate each $\\mu_k$ using the weighted average of the training examples\n",
    "\n",
    "$$\n",
    "\\mu_k \\leftarrow \\frac{\\textstyle\\sum_{i=1}^m r_{ik}x_i}{\\textstyle\\sum_{i=1}^m r_{ik}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: The following code skeleton will compute a single iteration of the EM algorithm given a current guess for the means, stored in $\\hat{\\mu}$ to distinguish them from the $\\mu$ used to simulate the data.  Your job is to complete the code for the E-Step and M-Step of the algorithm. \n",
    "\n",
    "\n",
    "**Hint**: Make sure you check the Helper Functions at the bottom.  You'll find some helpful things already implemented. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def EM(X, muh, sig=[0.5, 0.5]):\n",
    "    \n",
    "    # E-Step to estimate r_ik's \n",
    "    R = np.zeros((X.shape[0],len(muh)))\n",
    "    for kk, (m, s) in enumerate(zip(muh, sig)):\n",
    "        R[:,kk] = gaussianPDF(X, m, s)\n",
    "    R = R / np.sum(R, axis=1)[:,None]\n",
    "    \n",
    "    # M-Step to compute MLE of means \n",
    "    for kk in range(len(muh)):\n",
    "        muh[kk] = np.dot(R[:,kk], X) / np.sum(R[:,kk])\n",
    "        \n",
    "    return muh "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll define an initial guess for the $\\hat{\\mu}$'s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "muh = [-2.0, -1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've implemented the EM algorithm correctly, repeated shift-entering of the following cell should update the $\\hat{\\mu}$'s and show you your progress. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "muh = EM(X, muh)\n",
    "prob1Progress(X, mu, sig, muh=muh) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: About how many iterations does it take for EM to converge? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Can you find an initial guess that will cause EM to get stuck in a local minimum? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: What would you need to do to modify the code if you assume the variances are not fixed and identical? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: What would you need to do to modify the code if you assume that the gaussians are selected with different probabilities? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Problem 2: Relationship between K-Means and GMMs with EM \n",
    "***\n",
    "\n",
    "I claimed in the videos that K-Means and GMM/EM are equivalent in the case that you assume that the variances of the multivariate Gaussians are identical and assumed known and that the individual Gaussians are chosen uniformly.  In this exercise we'll prove that this is true.  \n",
    "\n",
    "**Q**: What is the objective function you're trying to minimize in K-Means? \n",
    "\n",
    "**A**: For a given set of examples $\\{{\\bf x}_i\\}_{i=1}^m$ and $k$ possible means $\\mu_k$ and cluster labels $\\{z_i\\}_{i=1}^m$, we have \n",
    "\n",
    "$$\n",
    "f(~{\\bf X}, z, \\mu) = \\sum_{k=1}^K \\sum_{i=1}^m I\\{z_i =k \\} \\|{\\bf x}_i - \\mu_k\\|^2\n",
    "$$\n",
    "\n",
    "**Q**: Given a specification of cluster membership for each ${\\bf x}_i$, how do we update the means of the clusters?\n",
    "\n",
    "**A**: To update $\\mu_k$ we simply take the mean of all ${\\bf x}_i$ assigned to cluster $k$\n",
    "\n",
    "$$\n",
    "\\mu_k = \\frac{\\textstyle\\sum_{i=1}^m I\\{{z_i=k}\\}{\\bf x}_i}{\\textstyle\\sum_{i=1}^m I\\{{z_i=k}\\}}\n",
    "$$\n",
    "\n",
    "**Q** Under the assumptions that $\\Sigma_k = \\Sigma = \\textrm{diag}(\\sigma^2, \\ldots, \\sigma^2)$ and that we've assigned examples $\\{{\\bf x}_i\\}_{i=1}^m$ to specific clusters, what is the log-likelihood of the parameters $\\mu_k$ and $\\Sigma_k$? **Hint**: It's easier if you start with the likelihood over examples in a particular cluster.  In other words, what is $\\ell(\\mu_k, \\Sigma_k ~|~ z = k)$? \n",
    "\n",
    "**A**: In general, we have \n",
    "\n",
    "$$\n",
    "\\ell(\\mu_k, \\Sigma_k ~|~ z = k) = \\sum_{i=1}^m I\\{z_i = k\\}\\log p({\\bf x}_i ~|~ z_i = k, \\mu_k, \\Sigma_k)\n",
    "$$\n",
    "\n",
    "where the pdf of the multivariate Gaussian for ${\\bf x}_i \\in \\mathbb{R}^n$ is given by \n",
    "\n",
    "$$\n",
    "p({\\bf x}_i ~|~ z_i = k, \\mu_k, \\Sigma_k) = \\frac{1}{(2\\pi)^{n/2} |\\Sigma_k|^{1/2}}\\exp\\left\\{-\\frac{1}{2}({\\bf x}_i - \\mu_k)^T\\Sigma_k^{-1}({\\bf x}_i - \\mu_k) \\right\\}\n",
    "$$\n",
    "\n",
    "Let's see how this simplifies under our assumptions.  First, the determinant of a diagonal matrix is simply the product of the main diagonal elements.  So we have \n",
    "\n",
    "$$\n",
    "|\\Sigma_k|^{1/2} = \\sqrt{\\sigma^{2n}} = \\sigma^n \\quad \\Rightarrow \\quad  \\frac{1}{(2\\pi)^{n/2} |\\Sigma_k|^{1/2}} = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\n",
    "$$\n",
    "\n",
    "The inverse of a diagonal matrix is simply a diagonal matrix with the reciprocals of the main diagonal elements (assuming none are zero), so we have \n",
    "\n",
    "$$\n",
    "\\Sigma_k^{-1} = \\textrm{diag}(1/\\sigma^2, \\ldots, 1/\\sigma^2)\n",
    "$$\n",
    "\n",
    "Multiplying this through in the exponent we see that \n",
    "\n",
    "$$\n",
    "-\\frac{1}{2}({\\bf x}_i - \\mu_k)^T\\Sigma_k^{-1}({\\bf x}_i - \\mu_k) = -\\frac{1}{2\\sigma^2}({\\bf x}_i - \\mu_k)^T({\\bf x}_i - \\mu_k)= -\\frac{1}{2\\sigma^2}\\|{\\bf x}_i - \\mu_k\\|^2\n",
    "$$\n",
    "\n",
    "So our density function becomes \n",
    "\n",
    "$$\n",
    "p({\\bf x}_i ~|~ z_i = k, \\mu_k, \\Sigma_k) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left\\{ -\\frac{1}{2\\sigma^2}\\|{\\bf x}_i - \\mu_k\\|^2   \\right\\}\n",
    "$$\n",
    "\n",
    "Plugging this into our cluster-$k$ log-likelihood, we have \n",
    "\n",
    "$$\n",
    "\\ell(\\mu_k, \\Sigma_k ~|~ z = k) = \\sum_{i=1}^m I\\{z_i = k\\}\\left\\{-\\log (2\\pi\\sigma^2)^{n/2}  -\\frac{1}{2\\sigma^2}\\|{\\bf x}_i - \\mu_k\\|^2  \\right\\}\n",
    "$$\n",
    "\n",
    "Since $\\sigma$ is assumed fixed, the first term drops out, and we have \n",
    "\n",
    "$$\n",
    "\\ell(\\mu_k, \\Sigma_k ~|~ z = k) \\propto -\\sum_{i=1}^m I\\{z_i = k\\}\\|{\\bf x}_i - \\mu_k\\|^2  \n",
    "$$\n",
    "\n",
    "Looking at the entire log-likelihood (over all $k$), we have \n",
    "\n",
    "$$\n",
    "\\ell(\\mu, \\Sigma ) \\propto -\\sum_{i=1}^m\\sum_{k=1}^K I\\{z_i = k\\}\\|{\\bf x}_i - \\mu_k\\|^2  \n",
    "$$\n",
    "\n",
    "Notice that this is just (proportional to, at least) the negative of the objective function we were minimizing in K-Means.  Thus, for fixed specifications of the $z_i$'s, minimizing the K-Means objective function is exactly equivalent to maximizing the log-likelihood in the EM algorithm. \n",
    "\n",
    "**Q**: What is the update rule for the means in the EM algorithm? \n",
    "\n",
    "**A**: From the lecture, we know that we're simply computing the MLE estimate of each $\\mu_k$ given that we have fixed cluster assignments.  We thus have \n",
    "\n",
    "$$\n",
    "\\mu_k = \\frac{\\textstyle\\sum_{i=1}^m I\\{{z_i=k}\\}{\\bf x}_i}{\\textstyle\\sum_{i=1}^m I\\{{z_i=k}\\}}\n",
    "$$\n",
    "\n",
    "exactly as in K-Means. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "\n",
    "<a id='helpers'></a>\n",
    "\n",
    "<br> \n",
    "\n",
    "### Helper Functions\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".MathJax nobr>span.math>span{border-left-width:0 !important};\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sn \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sn.set_style(\"white\")\n",
    "\n",
    "def prob1Data(N=40, mu=[-1, 1], sig=[.5, .5], seed=1234):\n",
    "    np.random.seed(seed)\n",
    "    X = np.zeros(N)\n",
    "    for ii in range(N):\n",
    "        # Flip coin to decide which normal to draw from\n",
    "        flip = int(np.round(np.random.uniform()))\n",
    "        # Draw from normal \n",
    "        X[ii] = np.random.normal(mu[flip], sig[flip])\n",
    "    return X \n",
    "\n",
    "def gaussianPDF(x, mu, sig):\n",
    "    # Evaluate the guassian pdf parameterized by mu and sig for a given x\n",
    "    return np.exp(-((x - mu)**2) / (2*sig*sig)) / np.sqrt(2*sig*sig*np.pi)\n",
    "    \n",
    "\n",
    "def prob1Progress(X, mu, sig, muh=None, sigh=None):\n",
    "    \n",
    "    # Make plots \n",
    "    mycolors = [\"steelblue\", \"#a76c6e\",  \"#6a9373\",  \"orange\", \"gray\"]\n",
    "    fig = plt.figure(figsize=(12,4))\n",
    "    ax = fig.add_subplot(111)\n",
    "        \n",
    "    # Plot true gaussians \n",
    "    x = np.linspace(np.min(X)-1,np.max(X)+2,400)\n",
    "    for ii, (m, s) in enumerate(zip(mu, sig)):\n",
    "        pdf = np.exp(-((x - m)**2) / (2*s)) / np.sqrt(2*s*np.pi)\n",
    "        ax.plot(x, pdf, lw=2, color=mycolors[ii], label=r\"$\\mu_{%s} = {%+.3g}$\"%(ii, m))\n",
    "        \n",
    "    # Plot data \n",
    "    ax.scatter(X, np.zeros(X.shape), color=\"gray\")\n",
    "    \n",
    "    # Plot estimated means \n",
    "    if np.any(muh): \n",
    "        for ii, m in enumerate(muh):\n",
    "            pdf = np.exp(-((x - m)**2) / (2*s)) / np.sqrt(2*s*np.pi)\n",
    "            ax.plot(x, pdf, lw=2, ls='--', color=mycolors[ii], label=r\"$\\mu_{%s} = {%+.3g}$\"%(ii, m))\n",
    "        \n",
    "    # Make legend \n",
    "    ax.legend(loc=\"upper right\", fontsize=16)\n",
    "        \n",
    "    # Make pretty \n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.set_xlim([np.min(x), np.max(x)])\n",
    "    ax.set_ylim([-0.25, 1.0])\n",
    "    plt.xticks([-2,-1,0,1,2,3], fontsize=12)\n",
    "    plt.yticks([], fontsize=16)\n",
    "    \n",
    "    \n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".MathJax nobr>span.math>span{border-left-width:0 !important};\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
